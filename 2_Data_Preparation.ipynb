{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Predictor - Data Preparation\n",
    "\n",
    "## Notebook Overview\n",
    "- [1. Load Modules and Data](#load-data)\n",
    "- [2. Model Descriptions](#model-descriptions)\n",
    "    - [2.1 ARIMA Model](#ARIMA-Model)\n",
    "    - [2.2 DeepAR Model](#DeepAR-Model)\n",
    "- [3. Multiple Time Series](#multiple-time-series)\n",
    "- [4. Split Data](#split-data)\n",
    "- [5. Saving Data](#save-in-json-format)\n",
    "    - [5.1 ARIMA Model](#save-arima-model)\n",
    "    - [5.2 DeepAR Model](#save-deepar-model)\n",
    "\n",
    "## Plan of Action\n",
    "The aim to prepare the data to be processed and save it as a `CSV` or `JSON` file. First any data processing will be carried out. Then data will be split into train and test data. Finally, this data will be saved in the folder called `data`. \n",
    "\n",
    "> *Note: ideally, data is split into training and test data, and then **separately** processed. However, as this is time series data, where the train-test split cannot be randomly selected points, processing first should not affect the results in any way.\n",
    "\n",
    "A `Adj Close` vs `Time` graph is to be created. This will be a line graph with the prediction of stock price for the next year (in the end: 2021). In this section data will be divided to be processed by each of the two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-data\"></a>\n",
    "# 1. Load Modules and Data\n",
    "All the required modules will be loaded here along with the data from the `CSV` files in the `data` directory files.\n",
    "\n",
    "> **Citation for data**: _Yahoo Finance â€“ stock market live, quotes, business &amp; finance news_ (no date). Available at: https://in.finance.yahoo.com/ (Accessed: 2 October 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "timezone_str = 'Asia/Kolkata'\n",
    "localtz = pytz.timezone(timezone_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stocks data\n",
    "stock_names = {'^GSPC': 'S&P 500',\n",
    "               '^BSESN': 'S&P BSE SENSEX',\n",
    "               'AAPL': 'Apple Inc.'}\n",
    "\n",
    "data_dir = 'data'\n",
    "data = {}\n",
    "\n",
    "for stock in stock_names.keys():\n",
    "    data[stock] = pd.read_csv(os.path.join(data_dir, stock + '.csv'),\n",
    "                              parse_dates=True, index_col=['Date'])\n",
    "    data[stock] = data[stock].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.101261</td>\n",
       "      <td>469033600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.122210</td>\n",
       "      <td>0.122210</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.095978</td>\n",
       "      <td>175884800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.112723</td>\n",
       "      <td>0.112723</td>\n",
       "      <td>0.088934</td>\n",
       "      <td>105728000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.115513</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>0.115513</td>\n",
       "      <td>0.115513</td>\n",
       "      <td>0.091135</td>\n",
       "      <td>86441600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.119420</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.093777</td>\n",
       "      <td>73449600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close       Volume\n",
       "Date                                                                      \n",
       "1980-12-12  0.128348  0.128906  0.128348  0.128348   0.101261  469033600.0\n",
       "1980-12-15  0.122210  0.122210  0.121652  0.121652   0.095978  175884800.0\n",
       "1980-12-16  0.113281  0.113281  0.112723  0.112723   0.088934  105728000.0\n",
       "1980-12-17  0.115513  0.116071  0.115513  0.115513   0.091135   86441600.0\n",
       "1980-12-18  0.118862  0.119420  0.118862  0.118862   0.093777   73449600.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['AAPL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE: The folowing sections will consider only `AAPL` stock data. Later, similar steps will be carried out on the two indices: `^GSPC` and `^BSESN`_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-descriptions\"></a>\n",
    "# 2. Model Descriptions\n",
    "In this section, we will discuss model types and how the models take data and process it. There are two models that will be used to make predictions and the results from both will later be compared.\n",
    "\n",
    "<a id=\"ARIMA-Model\"></a>\n",
    "## 2.1 ARIMA Model\n",
    "One of the major problems with time series problems are the non-stationary nature of most time series. To get around this obstacle, a time series is usually stationaized. This is done by reducing effects of trends and seasonality, training a model and finally converting the predicitons back (adding trends and seasonality). A complex method can be used to efficiently stationarize a time series, but it makes it harder to convert the results back. Though differencing is not the most efficient of stationarizing a time series, it provides an easy to remove and add trends and seasonality. Hence, ARIMA model is chosen for this project.\n",
    "\n",
    "ARIMA (Auto-Regressive Integrated Moving Average) utilizes differencing to stationarize a non-stationary time series. It takes in three parameters:\n",
    "1. Number of `auto-regressive terms` (p): The number of past terms, the current value depends on.\n",
    "2. Number of `differences` (d): Difference between two consecutive terms if `d=1`, higher degree takes differences between more terms.\n",
    "3. Number of `moving average terms` (q): The number of past error terms, the current value depends on.\n",
    "\n",
    "Determining optimal values of `p` and `q` is part of modelling ARIMA. Two plots are used to determine these values.\n",
    "1. Autocorrelation Function (ACF)\n",
    "2. Partial Autocorrelation Function (PACF)\n",
    "\n",
    "An ARIMA model takes a time series as training data. Furthermore, it can take a single long series of data. Therefore, there is not much data processing required. The data will be prepared here and the parameters `(p, d, q)` will be determined in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1980-12-12    0.101261\n",
       "1980-12-15    0.095978\n",
       "1980-12-16    0.088934\n",
       "1980-12-17    0.091135\n",
       "1980-12-18    0.093777\n",
       "Name: Adj Close, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adj Close Time series\n",
    "aapl_ts_adj = data['AAPL']['Adj Close'].copy()\n",
    "aapl_ts_adj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data will be created when required (like above) as it can be easily formed from the entire dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DeepAR-Model\"></a>\n",
    "## 2.2 DeepAR Model\n",
    "\n",
    "DeepAR is Amazon SageMaker's supervised learning algorithm that uses a recurrent neural network (RNN) to train on the data provided. The neural network trains on multiple time series of predefined length. It uses a **context length** to predict adjacent \"_prediction window_\". A \"_training example_\" is of the same length and is made up of context and prediction lengths.\n",
    "\n",
    "DeepAR accepts a `.json` file with training data. The format of the `JSON` file is as follow (*source: AWS website*):\n",
    "- `start` (str): timestamp of format `YYYY-MM-DD HH:MM:SS`.\n",
    "- `target` (array of floats): values in the time series.\n",
    "- `cat` (optional, integer): catergory for multivariate time series.\n",
    "\n",
    "> **Source**: Amazon Web Services, I. (no date) DeepAR Forecasting Algorithm - Amazon SageMaker. Available at: https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html (Accessed: 29 October 2020).\n",
    "\n",
    "\n",
    "Another important point about missing data is to be taken into consideration. In one of DeepAR's update<sup>1</sup>, it is mentioned that it supports missing data points. Hence, all the missing data can be included in the `target` array and DeepAR will take of them.\n",
    "\n",
    "> <sup>1</sup>Flunkert, V. et al. (2018) Amazon SageMaker DeepAR now supports missing values, categorical and time series features, and generalized frequencies | AWS Machine Learning Blog, Amazon SageMaker, Artificial Intelligence. Available at: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-deepar-now-supports-missing-values-categorical-and-time-series-features-and-generalized-frequencies/ (Accessed: 29 October 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>115.010002</td>\n",
       "      <td>115.320000</td>\n",
       "      <td>112.779999</td>\n",
       "      <td>114.959999</td>\n",
       "      <td>114.959999</td>\n",
       "      <td>137672400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>114.550003</td>\n",
       "      <td>115.309998</td>\n",
       "      <td>113.570000</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>99382200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>113.790001</td>\n",
       "      <td>117.260002</td>\n",
       "      <td>113.620003</td>\n",
       "      <td>115.809998</td>\n",
       "      <td>115.809998</td>\n",
       "      <td>142675200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>117.639999</td>\n",
       "      <td>117.720001</td>\n",
       "      <td>115.830002</td>\n",
       "      <td>116.790001</td>\n",
       "      <td>116.790001</td>\n",
       "      <td>116120400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "2020-09-27         NaN         NaN         NaN         NaN         NaN   \n",
       "2020-09-28  115.010002  115.320000  112.779999  114.959999  114.959999   \n",
       "2020-09-29  114.550003  115.309998  113.570000  114.089996  114.089996   \n",
       "2020-09-30  113.790001  117.260002  113.620003  115.809998  115.809998   \n",
       "2020-10-01  117.639999  117.720001  115.830002  116.790001  116.790001   \n",
       "\n",
       "                 Volume  \n",
       "2020-09-27          NaN  \n",
       "2020-09-28  137672400.0  \n",
       "2020-09-29   99382200.0  \n",
       "2020-09-30  142675200.0  \n",
       "2020-10-01  116120400.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding `NaN` for missing values.\n",
    "df = data['AAPL'].index\n",
    "idx = pd.date_range(min(df.date), max(df.date))\n",
    "aapl_updated = data['AAPL'].copy().reindex(idx)\n",
    "\n",
    "aapl_updated.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, we have filled all the missing dates with `NaN` values. Now let's create a function that can take a pandas series and split it into multiple smaller time series.\n",
    "\n",
    "<a id=\"multiple-time-series\"></a>\n",
    "# 3. Create Multiple Time Series - DeepAR\n",
    "The following function creates a list with different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series(data: pd.Series, series_length_years: int, \n",
    "                       start_date: datetime.datetime = None,\n",
    "                       last_date: datetime.datetime = None,\n",
    "                       equal_series: bool = True):\n",
    "    \"\"\"Creates a list of time series of the given length from the data provided.\n",
    "    Args:\n",
    "        data (pd.Series): pandas series with all the data, indexed with the timestamp.\n",
    "        series_length_years (int): length of each series in the list to be created.\n",
    "        start_date (datetime.datetime, optional): \n",
    "            Date the first time series should start from. If None, '2002-01-01' is used.\n",
    "            Defaults to None.\n",
    "        last_date (datetime.datetime, optional):\n",
    "            Date the last time series should end on. If None, last date found in `data` will be used.\n",
    "            Defaults to None.\n",
    "        equal_series (bool): if True, all series created will be of equal length. last series created\n",
    "                             will be removed if it is shorter than the others.\n",
    "    \n",
    "    Returns:\n",
    "        (list of pd.Series): python list containing all the time series created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Updating dictionaries.\n",
    "    if start_date is None:\n",
    "        start_date = datetime.datetime(2002, 1, 1)\n",
    "    if last_date is None:\n",
    "        last_date = max(data.index.date)\n",
    "    else:\n",
    "        last_date = last_date.date()\n",
    "    \n",
    "    start_date = start_date.date()\n",
    "    time_series_list = []\n",
    "\n",
    "    while start_date < last_date:\n",
    "        end_date = start_date + pd.DateOffset(years=series_length_years) - pd.DateOffset(days=1)\n",
    "        time_series_list.append(data.loc[start_date:end_date])\n",
    "        start_date = end_date + pd.DateOffset(days=1)\n",
    "        \n",
    "    is_last_equal = str(max(time_series_list[0].index.date))[5:10] == str(max(time_series_list[-1].index.date))[5:10]\n",
    "    \n",
    "    if equal_series and not is_last_equal:\n",
    "        time_series_list = time_series_list[:-1]\n",
    "    \n",
    "    print(f'Number of series created: {len(time_series_list)}')\n",
    "    print(f'Last series removed: {not is_last_equal}')\n",
    "    print(f'Last series end date: {max(time_series_list[-1].index.date)}')\n",
    "    return time_series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of series created: 6\n",
      "Last series removed: True\n",
      "Last series end date: 2019-12-31\n"
     ]
    }
   ],
   "source": [
    "ts_list = create_time_series(aapl_updated['Adj Close'], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-data\"></a>\n",
    "# 4. Split Data\n",
    "The train-test split being created for DeepAR can also be used for the ARIMA model. However, it ARIMA model would do better if provided with a larger dataset. Maintaining the prediction length for both the models, the data will be split differently. ARIMA model will be provided one a **single set** of train time series and test time series, while multiple time series data will be provided to DeepAR model to train and then its performance will be tested on the last 10 months of data (Jan 2020 - Oct 2020). This will ensure both models can be compared well.\n",
    "\n",
    "<a id=\"split-arima-model\"></a>\n",
    "## 4.1 ARIMA model\n",
    "The data from years `2002` to `2019` will be split into train and test data. The prediction length is equal to `10 months`. Furthermore, all the missing values in the test time series will be interpolated after the ARIMA model is trained and has made predictions (before metric calculation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ts date range:  2002-01-01 - 2019-12-31\n",
      "Train ts date range: 2002-01-01 - 2019-02-28\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime(2002, 1, 1)\n",
    "end_test = datetime.datetime(2019, 12, 31)\n",
    "end_train = end_test - pd.DateOffset(months=10)\n",
    "\n",
    "arima_test = aapl_updated['Adj Close'][start:end_test]\n",
    "arima_train = aapl_updated['Adj Close'][start:end_train]\n",
    "\n",
    "print(f'Test ts date range:  {min(arima_test.index.date)} - {max(arima_test.index.date)}\\n'\n",
    "      f'Train ts date range: {min(arima_train.index.date)} - {max(arima_train.index.date)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-deepar-model\"></a>\n",
    "## 4.2 DeepAR model\n",
    "\n",
    "The following function takes a list of time series and prediction length and return a list of training time series. This is will ensure we can format the data for the DeepAR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_series(time_series_list: list,\n",
    "                           prediction_length_months: int):\n",
    "    \"\"\"Create a training series using the prediction length provided in months.\n",
    "    \n",
    "    Args:\n",
    "        time_series_list (list): list of pandas series each of equal length.\n",
    "        prediciton_length_months (int): number of months a prediction is to be made. This will be\n",
    "                                        used to created a training series, i.e. context length for\n",
    "                                        DeepAR algorithm to train on.\n",
    "    \n",
    "    Returns:\n",
    "        list of pd.Series: python list containing the training time series.\n",
    "    \"\"\"\n",
    "    training_series_list = []\n",
    "    \n",
    "    for ts in time_series_list:\n",
    "        end = max(ts.index.date) - pd.DateOffset(months=prediction_length_months)\n",
    "        training_series_list.append(ts[:end])\n",
    "\n",
    "    print(f'Number of series updated: {len(training_series_list)}')\n",
    "    return training_series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of series updated: 6\n"
     ]
    }
   ],
   "source": [
    "# Create training series\n",
    "prediction_length = 10\n",
    "train_series_list = create_training_series(ts_list, prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save-in-json-format\"></a>\n",
    "# 5. Saving Data\n",
    "\n",
    "<a id=\"save-arima-model\"></a>\n",
    "## 5.1 ARIMA Model\n",
    "The train and test data will be saved to a csv file for the ARIMA model as there is no particular format the algorith accepts the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data/csv_aapl_data with train-test data.\n"
     ]
    }
   ],
   "source": [
    "csv_data_dir = 'data/csv_aapl_data'\n",
    "\n",
    "if not os.path.exists(csv_data_dir):\n",
    "    os.makedirs(csv_data_dir)\n",
    "    \n",
    "arima_train.to_csv(os.path.join(csv_data_dir, f'train.csv'))\n",
    "arima_test.to_csv(os.path.join(csv_data_dir, f'test.csv'))\n",
    "\n",
    "print(f'Created {csv_data_dir} with train-test data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save-deepar-model\"></a>\n",
    "## 5.2 DeepAR Model\n",
    "Amazon SageMaker's DeepAR model accepts data in through a JSON file. The following function will create a file for the list of time series input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_series_to_json(time_series_list: list,\n",
    "                        filename: str,\n",
    "                        data_dir: str = 'json_time_series_data'):\n",
    "    \"\"\"Function takes a list of time series data and then saves in DeepAR, JSON format.\n",
    "    \n",
    "    Args:\n",
    "        time_series_list (list): list of pandas series each of equal length.\n",
    "        filename (str): name of the file that will contain the data in DeepAR, JSON format.\n",
    "        data_dir (str, optional): name of directory that will hold the JSON files.\n",
    "    \n",
    "    Returns:\n",
    "        str: path to the file created.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        for ts in time_series_list:\n",
    "            line = json.dumps({\n",
    "                \"start\": str(ts.index[0]),\n",
    "                \"target\": ts.interpolate().dropna().tolist()}) + '\\n'\n",
    "            json_line = line.encode('utf-8')\n",
    "            f.write(json_line)\n",
    "    print(f'{file_path} created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/json_aapl_data/train.json created.\n",
      "data/json_aapl_data/test.json created.\n"
     ]
    }
   ],
   "source": [
    "json_data_dir = 'data/json_aapl_data'\n",
    "\n",
    "train_data = save_series_to_json(train_series_list, 'train.json', json_data_dir)\n",
    "test_data = save_series_to_json(ts_list, 'test.json', json_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
